{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb52dd1f-086a-4b11-94f9-45ccf4283e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c150060-79f6-45b7-a21d-953bc11085d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countPipes(line):\n",
    "    return line.count('|') + bool(re.search(r\"\\+-\\(\\d+\\)\", line))\n",
    "\n",
    "\n",
    "\n",
    "def complete_and_save(debugLines):\n",
    "    branches = 1 # we assusme that there is at least one branch\n",
    "    id = 1\n",
    "    result = []\n",
    "    split_ids = []\n",
    "    \n",
    "    for line in reversed(debugLines):\n",
    "        if (\"MapPartitionsRDD\" in line and not (\"fullOuterJoin\" in line or \"join\" in line or \"sortBy\" in line or \"leftOuterJoin\" in line or \"rightOuterJoin\" in line)) or (\"ShuffledRDD\" in line or \"CoGroupedRDD\" in line):\n",
    "            pipesInLine = countPipes(line)\n",
    "            if (pipesInLine == branches) or (pipesInLine == 0):\n",
    "                #data source\n",
    "                if (id-1 == 0):\n",
    "                    result.append({'id': id, 'type': 'Data Source', 'operator': re.search(r\"at (.+) at\", line).group(1)})\n",
    "                    id += 1\n",
    "                elif (pipesInLine != 0):\n",
    "                    result.append({'id': id, 'type': 'operator', 'operator': re.search(r\"at (.+) at\", line).group(1), 'predecessors': [{'id': id-1}]})\n",
    "                    id += 1\n",
    "                #data sink\n",
    "                else:\n",
    "                    result.append({'id': id, 'type': 'operator', 'operator': re.search(r\"at (.+) at\", line).group(1), 'predecessors': [{'id': id-1}]})\n",
    "                    id += 1\n",
    "                    result.append({'id': id, 'type': 'sink', 'operator': 'Data Sink', 'predecessors': [{'id': id-1}]})\n",
    "                    id += 1\n",
    "            else:\n",
    "                #starting new branch\n",
    "                if pipesInLine > branches:\n",
    "                    split_ids.append(id-1)\n",
    "                    result.append({'id': id, 'type': 'Data Source', 'operator': re.search(r\"at (.+) at\", line).group(1)})\n",
    "                    id += 1\n",
    "                    branches += 1\n",
    "\n",
    "                    \n",
    "                #merging branches, but check for +-(d+) case\n",
    "                else:\n",
    "                    if bool(re.search(r\"\\+-\\(\\d+\\)\", line)):\n",
    "                        result.append({'id': id, 'type': 'operator', 'operator': re.search(r\"at (.+) at\", line).group(1), 'predecessors': [{'id': id-1}]})\n",
    "                        id += 1\n",
    "                    else:\n",
    "                        result.append({'id': id, 'type': 'operator', 'operator': re.search(r\"at (.+) at\", line).group(1), 'predecessors': [{'id': id-1}, {'id': split_ids.pop()}]})\n",
    "                        id += 1\n",
    "                        branches -= 1\n",
    "                        \n",
    "    #check if Data Sink was added(can be not, if first line is MapPartitionsRDD with sortBy)\n",
    "    if result[-1]['type'] != 'sink':\n",
    "        result.append({'id': id, 'type': 'sink', 'operator': 'Data Sink', 'predecessors': [{'id': id-1}]})\n",
    "\n",
    "        \n",
    "        \n",
    "    return {\"executionPlan\" : {\"nodes\" : result}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a25f43c-ff23-45d0-8dd8-e7bc89dd23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'debugStrings'\n",
    "output_dir = 'unified_input'\n",
    "\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        debugString = open(f, \"r\")\n",
    "        debugLines = debugString.readlines()\n",
    "        \n",
    "        \n",
    "        unified_json = complete_and_save(debugLines)\n",
    "        #print(*unified_json, sep=\"\\n\")\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'TPC_H_' + filename.split('.')[0] + 'Unified.json'), 'w', encoding='utf-8') as nf:\n",
    "            json.dump(unified_json, nf, ensure_ascii=False, indent=2)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
